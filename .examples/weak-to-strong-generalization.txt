<document>Notes for W EAK - TO -S TRONG G ENERALIZATION : E LICITING

...
A BSTRACT
Widely used alignment techniques, such as reinforcement learning from human
feedback (RLHF), rely on the ability of humans to supervise model behavior—for
example, to evaluate whether a model faithfully followed instructions or generated
safe outputs. However, future superhuman models will behave in complex ways
too difficult for humans to reliably evaluate; humans will only be able to weakly
supervise superhuman models. We study an analogy to this problem: can weak
model supervision elicit the full capabilities of a much stronger model? We test
this using a range of pretrained language models in the GPT-4 family on natural
language processing (NLP), chess, and reward modeling tasks. We find that when
we naively finetune strong pretrained models on labels generated by a weak model,
they consistently perform better than their weak supervisors, a phenomenon we
call weak-to-strong generalization. However, we are still far from recovering the
full capabilities of strong models with naive finetuning alone, suggesting that tech-
niques like RLHF may scale poorly to superhuman models without further work.
We find that simple methods can often significantly improve weak-to-strong gen-
eralization: for example, when finetuning GPT-4 with a GPT-2-level supervisor
and an auxiliary confidence loss, we can recover close to GPT-3.5-level perfor-
mance on NLP tasks. Our results suggest that it is feasible to make empirical
progress today on a fundamental challenge of aligning superhuman models.

...
We mainly steer or align today’s models with reinforcement learning from human feedback (RLHF):
we reinforce behaviors that human evaluators rate highly and penalize behaviors that evaluators rate
poorly  This procedure is very effective when human evaluators can tell if model behavior is
good or bad and is a core part of training modern language model assistants such as ChatGPT.


...
However, superhuman models will be capable of complex and creative behaviors that humans can-
not fully understand. For example, if a superhuman assistant model generates a million lines of ex-
tremely complicated code, humans will not be able to provide reliable supervision for key alignment-
relevant tasks, including: whether the code follows the user’s intentions, whether the assistant model
answers questions about the code honestly, whether the code is safe or dangerous to execute, and
so on. As a result, if we finetune a superhuman model with human supervision on a reward mod-
eling (RM) or safety classification task, it is unclear how that model will generalize to complicated
behaviors that humans could not reliably supervise themselves.
This leads to a fundamental technical challenge of aligning superhuman models (superalignment):
how can weak supervisors control models much smarter than them?

...
 Despite the importance of
∗ this problem, it is difficult to empirically study today.


...
 Most prior work on alignment has either
confronted this core challenge head-on—but been restricted to primarily theoretical frameworks and
toy problems  or empirically studied humans supervising today’s models—without
addressing the core challenges that may arise with superhuman models


...
 In contrast, we would
ideally like to have a setup that captures core challenges of aligning future superhuman models while
also being able to make iterative empirical progress today.

...
https://i.imgur.com/8ggevc0.png

...
Figure 1: An illustration of our methodology. Traditional ML focuses on the setting where humans
supervise models that are weaker than humans. For the ultimate superalignment problem, humans
will have to supervise models much smarter than them. We study an analogous problem today:
using weak models to supervise strong models.

...
We propose a simple setup for studying the problem of humans supervising superhuman models by
considering an analogy: can we use weak models to supervise strong models? We can empirically
test this by finetuning large (strong) pretrained models on labels generated by small (weak) mod-
els and observing how they generalize. Just like the problem of humans supervising superhuman
models, our setup is an instance of what we call the weak-to-strong learning problem.

...
Why should weak-to-strong learning be possible? On the one hand, the strong model could simply
learn to imitate the weak supervisor, including its errors, since that is what we would naively train
it to do. On the other hand, strong pretrained models should already have good representations of
the alignment-relevant tasks we care about. For example, if a model can generate complicated code,
then it should intuitively also know whether that code faithfully adheres to the user’s instructions.
As a result, for the purposes of alignment we do not need the weak supervisor to teach the strong
model new capabilities; instead, we simply need the weak supervisor to elicit what the strong model
already knows. This gives us hope that the strong model can generalize beyond the weak supervision,
solving even hard problems for which the weak supervisor can only give incomplete or flawed
training labels. We call this phenomenon weak-to-strong generalization.

...
We study our weak-to-strong learning setup (Section 3) by finetuning base (i.e. pretrained-only)
language models from the GPT-4 family (OpenAI, 2023),1 spanning 7 orders of magnitude (OOMs)
of pretraining compute, across three settings: a large set of popular natural language processing
(NLP) benchmarks, chess puzzles, and our internal ChatGPT reward modeling dataset.

...
1
These models share the same general architecture and pretraining dataset as GPT-4. However, this model
series does not include the models known as GPT-2, GPT-3, and GPT-3.5.

...
Our main
findings include: Our main
findings include:

...
<subject>1. Strong pretrained models naturally generalize beyond their weak supervisors. If we
naively finetune strong models with labels generated by weak models, they consistently
outperform their weak supervisors (Section 4.2). For example, on NLP tasks, if we fine-
tune GPT-4 with labels from a GPT-2-level model, we typically recover about half of the
performance gap between the two models.</subject>
<question>I do not understand why it is surprising that strong models outperform weaker models in this context</question>


...
2. Naively finetuning on weak supervison is not enough. Despite positive weak-to-strong
generalization, there still remains a substantial gap between strong models finetuned with
weak supervision and strong models finetuned with ground truth supervision. Weak-to-
strong generalization is particularly poor for ChatGPT reward modeling. Collectively, our
results provide empirical evidence that naive RLHF will likely scale poorly to superhuman
models without additional work.
3. Improving weak-to-strong generalization is tractable. We find that we can improve per-
formance by encouraging strong models to have confident predictions with an auxiliary
loss, bootstrapping supervision with intermediate models, and improving model represen-
tations with unsupervised finetuning. For example, when supervising GPT-4 with a GPT-2-
level model on NLP tasks using the auxiliary confidence loss, we typically recover nearly
80% of the performance gap between the weak and strong models.

...
<subject>when supervising GPT-4 with a GPT-2-
level model on NLP tasks using the auxiliary confidence loss, we typically recover nearly
80% of the performance gap between the weak and strong models.</subject>
<question>I do not understand this, it sounds like they are training the large model to recover 80% of the gap between the strong model.</question>


...
We show that sub-
stantial weak-to-strong generalization is not only possible, but actually a widespread phenomenon.
We also show that with very simple methods, we can drastically improve the ability of weak super-
visors to elicit knowledge from strong models. With much more progress in this direction, we could
get to the point where we can use weak supervisors to reliably elicit knowledge from much stronger
3
models, at least for some key tasks that we care about. This may allow us to develop superhuman
reward models or safety classifiers, which we could in turn use to align superhuman models.
Aligning superhuman models is essential for making them safe; there is increasing recognition that
failing to align such powerful models has the potential to be catastrophic, making this one of the
most important unsolved technical problems in the world (CAIS, 2022). We think it is now more
tractable than ever to make rapid iterative empirical progress toward solving this problem.

...
2
 R ELATED WORK

...
Weakly-supervised learning. Weak-to-strong learning is a special type of weakly supervised
learning—a setting in which models are trained using unreliable labels There is also a rich literature on the related problem of learning
from noisy labels (Song et al., 2022). 


...
 Unlike most work on label noise, the
errors in our weak supervision are much harder to address than uniform label noise, instead having
“instance-dependent” errors (Frénay & Verleysen, 2013).

...
# <title>Learning from Noisy Labels with Deep Neural Networks: A Survey</title>

## <abstract>Abstract</abstract>
Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we first describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of <co: 0,2,3,4>62</co: 0,2,3,4> <co: 0,2,3,4>state-of-the-art robust training methods</co: 0,2,3,4>, all of which are categorized into <co: 1,3>five</co: 1,3> <co: 0,1,2,3,4>groups according to their methodological difference</co: 0,1,2,3,4>, followed by a <co: 0,2,3,4>systematic comparison of six properties used to evaluate their superiority.</co: 0,2,3,4> Subsequently, we <co: 0,4>perform an in-depth analysis of noise rate estimation</co: 0,4> and <co: 0,3,4>summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics.</co: 0,3,4> Finally, we <co: 0,3,4>present several promising research directions that can serve as a guideline for future studies.</co: 0,3,4>


...
We could also study our problem in a semi-supervised setting by
having an “easy” subset of examples that weak supervisors provide reliable labels for and a subset
of unlabeled “hard” examples that the weak supervisor can’t reliably label, a problem which we call
“easy-to-hard generalization” (see Appendix C).

...
Student-teacher training. The framework of first training a teacher and then training a student on
teacher’s pseudo-labels is widely used in semi-supervised learning domain adaptation


...
 and
knowledge distillation (Hinton et al., 2015; Gou et al., 2021; Stanton et al., 2021; Beyer et al., 2022).

...
 compared to most past work we are focused on qualitatively very
weak supervision. For example, we are interested in huge leaps in generalization, similar to going
from “3rd grade-level” supervisors to “12th grade-level” student models. Despite these differences
with past work, we expect many methods from semi-supervised learning and domain adaptation to
translate to our setting.

...
Robustness of pretraining and finetuning.
 Many papers have shown that pretraining
on massive, diverse data leads to more robust representations that generalize better out-of-
distribution Finetuning typ-
ically improves in-distribution generalization, but often performs poorly out-of-distribution, some-
times even degrading performance relative to zero-shot prompting (Kumar et al., 2022; Wortsman
et al., 2022b; Awadalla et al., 2022).


...
# <title>Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution</title>

## <abstract>Abstract</abstract>
When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR → STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).
## 

## <url>https://arxiv.org/abs/2202.10054</url>


...
<title>Exploring The Landscape of Distributional Robustness for Question Answering Models</title>

<abstract>We conduct a large empirical evaluation to investigate the landscape of distributional robustness in question answering. Our investigation spans over 350 models and 16 question answering datasets, including a diverse set of architectures, model sizes, and adaptation methods (e.g., fine-tuning, adapter tuning, in-context learning, etc.). We find that, in many cases, model variations do not affect robustness and in-distribution performance alone determines out-of-distribution performance. Moreover, our findings indicate that i) zero-shot and in-context learning methods are more robust to distribution shifts than fully fine-tuned models; ii) few-shot prompt fine-tuned models exhibit better robustness than few-shot fine-tuned span prediction models; iii) parameter-efficient and robustness enhancing training methods provide no significant robustness improvements. In addition, we publicly release all evaluations to encourage researchers to further analyze robustness trends for question answering models.</abstract>

<url>https://arxiv.org/abs/2210.12517</url>


...
Debiasing. In weak-to-strong generalization, the weak labels contain a specific form of bias,
which results from the weak models’ lack of capability. There is a substantial literature on learning
from biased training data 

...
However, most work focuses on known biases,
for example where we know that the models perform worse on minority groups. For known biases,
common methods include Group Distributionally Robust Optimization (Sagawa et al., 2019), adver- sarial training (Zhang et al., 2018), and model editing (Santurkar et al., 2021; Meng et al., 2022).


...
In contrast, our setting can be viewed as a particularly difficult debiasing problem where the bias is
unknown. Some methods that automatically discover and mitigate biases include clustering (Sohoni
et al., 2020), loss variance reduction (Khani et al., 2019), and auditing and re-training on high-loss
group (Kim et al., 2019; Liu et al., 2021).

...
Imitation and preference learning. The goal of alignment is to steer already-capable models
to do what we want them to do. For example, the base GPT-4 model is good at generating text
following its pretraining distribution, but does not readily follow instructions. To align pretrained
language models today, we finetune them using imitation learning on human demonstrations or by using methods such as reinforcement learning
from human feedback (RLHF) 


...
 Constitutional AI (Bai et al., 2022b; Lee et al., 2023) leverages
AI feedback to align language models, but still uses an initial RLHF phase. However, both imitation
learning and preference learning assume high-quality human supervision, making it unclear if they
will work for superhuman models.

...
<subject>The goal of alignment is to steer already-capable models
to do what we want them to do. For example, the base GPT-4 model is good at generating text
following its pretraining distribution, but does not readily follow instructions. </subject>
<reader_note>Important definition</reader_note>


...
<title>Training language models to follow instructions with human feedback</title>

<abstract>Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.</abstract>

<url>https://arxiv.org/abs/2203.02155</url>


...
Scalable oversight. Scalable oversight techniques aim to improve the ability of humans to super-
vise models. For example, humans may ask models to critique the outputs of other models   or use models to help decompose a problem into simpler sub-
problems


...
<title>Self-critiquing models for assisting human evaluators</title>

<abstract>We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.</abstract>

<url>https://arxiv.org/abs/2206.05802</url>


...
<title>Let's Verify Step by Step</title>

<abstract>In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.</abstract>

<url>https://arxiv.org/abs/2305.20050</url>


...
Scalable oversight
methods typically take advantage of special problem structure, like decomposability or the fact that
evaluation is easier than generation.

...
<subject>Scalable oversight
methods typically take advantage of special problem structure, like decomposability or the fact that
evaluation is easier than generation.</subject>
<reader_note></reader_note>


...
 In contrast to improving human supervision, we focus on gener-
alizing beyond human supervision such that models perform well even in settings we cannot reliably
supervise. 

...
. Our setup also resembles a proposal for measur-
ing progress on scalable oversight known as “sandwiching”, which uses weak and strong humans
(Cotra, 2021; Bowman, 2022).

...
## <title>The case for aligning narrowly superhuman models</title>

## <abstract>I wrote this post to get people’s takes on a type of work that seems exciting to me personally; I’m not speaking for Open Phil as a whole. Institutionally, we are very uncertain whether to prioritize this (and if we do where it should be housed and how our giving should be structured). We are not seeking grant applications on this topic right now.</abstract>

## <url>https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models</url>


...
## <title>Artificial Sandwiching: When can we test scalable alignment protocols without humans</title>?

## <abstract>Background: Standard sandwiching (in my terms)</abstract>

<abstract>Artificial sandwiching: Replacing the non-expert human</abstract>

<abstract>Epistemic status: Not a fleshed-out proposal. Brainstorming/eliciting ideas.</abstract>

<abstract>Thanks to Ben Mann, Pablo Moreno, and Jared Kaplan for feedback on early drafts.</abstract>

<abstract>I’m convinced sandwiching—the experimental protocol from Ajeya Cotra’s The case for aligning narrowly superhuman models—is valuable, and I’m in the process of setting up some concrete sandwiching experiments to test scalable oversight ideas.</abstract>

<abstract>Sandwiching experiments are generally fairly slow</abstract>:

- <abstract>You have to design and pilot a strategy that allows humans to use (or oversee) a model for a task that they can’t do well themselves. The details matter here, and this can often take many iterations to get right.</abstract>

- <abstract>Then, you need a bunch of humans actually try this. Even for very simple tasks, this is a high-cognitive-load task that should take at least tens of minutes per instance.</abstract>

- <abstract>You have to repeat this enough times to measure average performance accurately.</abstract>

<abstract>I’m visiting Anthropic this year for a sabbatical, and some of my sandwiching work is happening there. Anthropic’s biggest comparative advantage (like that of similar teams at DeepMind and OpenAI) is easy access to near-state-of-the-art LMs that are fine-tuned to be helpful dialog agents.</abstract>

<abstract>In that context, I've heard or encountered this question several times: Can we speed up [some experiment I’m proposing] by replacing the non-expert human with a weaker LM</abstract>?

<abstract>This obviously doesn’t achieve the full aims of sandwiching in general, but it’s often hard to find a decisive rebuttal for these individual instances.</abstract>

<abstract>More broadly, I think there’s likely to be a significant subset of worthwhile sandwiching experiments that can be trialed more quickly by using an intentionally weakened model as a proxy for the human.</abstract>

<abstract>Which experiments these are, precisely, has been hard for me to pin down. This post is an attempt to organize my thoughts and solicit comments.</abstract>

## <abstract>Background: Standard sandwiching (in my terms)</abstract>

<abstract>A hard task: A task that many humans would be unable to solve on their own.</abstract>

<abstract>A capable but misaligned language model assistant: A model that appears to have the skills and knowledge needed to solve the task better than many humans, but that does not reliably do so when prompted.</abstract>

<abstract>A non-expert human: Someone who can’t solve the task on their own, but will try to solve it using the assistant and some scalable alignment strategy.</abstract>

<abstract> [Secondary] Expert human: Someone who can solve the task well, and represents a benchmark for success.</abstract>

<abstract>In many cases, we’ll just measure accuracy with static test datasets/metrics rather than bringing in experts at experiment time.</abstract>

<abstract>My framing (minimum viable experiment)</abstract>:

<abstract>Search for scalable alignment protocols that allow the non-expert human to use or train the assistant to do as well as possible on the task.</abstract>

<abstract>Alternate framing (more steps, closer to the original blog post)</abstract>:

<abstract>Search for scalable alignment protocols by which the non-expert human can train the assistant to perform the task.</abstract>

<abstract>Run the same protocol with the expert human, and verify that the results are the same. This demonstrates successful (prosaic) alignment for the given assistant and task.</abstract>

<abstract>Example (task, non-expert human) pairs</abstract>:

- <abstract>Try to get a human with no medical qualifications to use a GPT-3-style assistant for medical advice, then check the advice with a doctor.</abstract>

- <abstract>Try to get a human who is working under moderate time constraints to use the assistant to answer exam questions from fields they’ve never studied.</abstract>

- <abstract>Try to get a human who is working under tight time constraints to use the assistant to answer questions about long pieces of fiction that they haven’t read.</abstract>

- <abstract>Try to get a human who has very limited programming experience to use the assistant to evaluate whether a piece of code is correct.</abstract>

<abstract>Get evidence about how scalable alignment strategies succeed and fail, as early as possible</abstract>

<abstract>Get practice trying to actually align systems in a maximally realistic setting</abstract>

<abstract>Setting up experiments with actual human annotators—at least at a large enough scale to get clear quantitative result—usually takes several weeks.</abstract>

<abstract>Current models will be misaligned in different ways from potentially-transformative models. Many apparent alignment failures will actually be largely due to capabilities gaps.</abstract>

## <abstract>Artificial sandwiching: Replacing the non-expert human</abstract>

<abstract>Non-expert model: A second language model which we constrain in some way such that it can’t solve the task on its own.</abstract>

<abstract>Example experiment(s)</abstract>:

<abstract>Debate: Use simple prompting techniques to get a small dialogue model to judge a debate between two more capable ones on MMMLU exam questions, where one of the debaters is trained or prompted to advocate for the wrong answer. Better debate protocols should yield higher success rates.</abstract>

<abstract>Short-context/long-context: Use simple prompting techniques to get a dialog model with a short context-length limit to interact with a longer-context assistant model in order to answer a question about a long piece of writing that they can both access. Make it clear in the prompt that the long-context model isn't always consistent or trustworthy, and that the short-context model will have to cross-examine it extensively.</abstract>

<abstract>Differences/results of this substitution</abstract>:

<abstract>Experiments are much faster: Once we find a promising basic setup, we can test out new strategies in only the few hours it takes to run a workflow with a large model.</abstract>

<abstract>We slightly expand the set of tasks we can work with: We can now use any task that some constrained/weakened LM is unable to solve on its own. (Though the LM still needs to be good enough to make a real attempt at playing the human role in some scalable alignment strategy.)</abstract>

<abstract>We don’t get any practice in the logistics of working with real humans.</abstract>

<abstract>We don’t get any practice with the specific types of goal misalignments that emerge between humans and LM-based agents.</abstract>

<abstract>Are there other important differences or limitations that should color which experiments we try and how we interpret the results</abstract>?

<abstract>Key difficulty/uncertainty</abstract>:

<abstract>It’s less clear that the two participants in our alignment strategy are actually misaligned: If we simply use smaller and larger variants of the same present-day language model dialog agent, is there an interesting sense in which they have different goals</abstract>?

<abstract>If the difference between the two is only one of capabilities, then the correct strategy for the weaker non-expert model is trivial: It should always just ask the assistant to do the task for them, while making no attempt at oversight.</abstract>

<abstract>We may need to inject misalignment manually</abstract>:

<abstract>Many debate protocols do this naturally: The non-expert model is interacting with a larger assistant which is sometimes explicitly prompted or incentivized to be misleading.</abstract>

<abstract>This only lets us test scalable alignment protocols that involve explicitly adversarial/zero-sum debates, though.</abstract>

<abstract>Is there an alternative strategy for misaligning the two models that lets us test a wider range of strategies</abstract>?

<abstract>The short-context/long-context example above attempts a simple version of this, but it's likely to be brittle.</abstract>

<abstract>Will we learn much without an explicit source of misalignment between the two? If we simply prompt/train a smaller model to lean on a larger one as a resource, can we learn anything important from those results</abstract>?

<abstract>To the extent that artificial sandwiching can get off the ground, it offers a pretty valuable tool: It lets you sanity check alignment strategies much, much more quickly.</abstract>

<abstract>I’m still excited about standard sandwiching, and it’ll be the priority in my own work: At some point, most plausible alignment processes will involve at least some input from actual human stakeholders, and we currently have relatively little practice at collecting and using that input.</abstract>

<abstract>If you want to optimally deploy your efforts on standard sandwiching, it’ll be valuable to understand where you expect artificial sandwiching to work.</abstract>


...
Knowledge elicitation and honesty. Christiano et al. (2022) introduced a theoretical problem
called Eliciting Latent Knowledge (ELK), in which the goal is to elicit latent knowledge from a su-
perhuman machine learning model even under worst case assumptions. For example, a special case
of ELK is honesty (Evans et al., 2021), where the goal is for the models to report their true beliefs2 

...
# <title>Eliciting latent knowledge: How to tell if your eyes deceive you</title>

## <abstract>ELK stands for Eliciting Latent Knowledge. ELK seems to capture a core difficulty in alignment. The short description of the issue captured by the problem is that we don’t have surefire ways to understand the beliefs of models and systems that we train, and so if we’re ever in a situation where our systems know things that we don’t, we can’t be sure that we can recover that information.</abstract>

<url: https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit?pli=1>https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit?pli=1</url>


...
<title>Benchmarks for Detecting Measurement Tampering</title>

<abstract>When training powerful AI systems to perform complex tasks, it may be challenging to provide training signals which are robust to optimization. One concern is <italic>measurement tampering</italic>, where the AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome. In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models. Concretely, given sets of text inputs and measurements aimed at determining if some outcome occurred, as well as a base model able to accurately predict measurements, the goal is to determine if examples where all measurements indicate the outcome occurred actually had the outcome occur, or if this was caused by measurement tampering. We demonstrate techniques that outperform simple baselines on most datasets, but don't achieve maximum performance. We believe there is significant room for improvement for both techniques and datasets, and we are excited for future work tackling measurement tampering.</abstract>

<url>https://arxiv.org/abs/2308.15605</url>


...
3
 M ETHODOLOGY

...
A core challenge of superalignment is that humans will need to supervise models much smarter than
us. This is a special case of what we call the weak-to-strong learning problem: how can a weak
supervisor oversee a model much smarter than it? In this paper, we study a simple analogy, in which
we replace the weak human supervisor with a weak model supervisor.
For a given task of interest, consisting of a dataset and a performance metric, we:
1. Create the weak supervisor. Throughout most of this work, we create weak supervisors
by finetuning small pretrained models on ground truth labels.3 We call the performance
of the weak supervisor the weak performance, and we generate weak labels by taking the
weak model’s predictions on a held-out set of examples.
2. Train a strong student model with weak supervision. We finetune a strong model with
the generated weak labels. We call this model the strong student model and its resulting
performance the weak-to-strong performance. 3. Train a strong model with ground truth labels as a ceiling. Finally, for comparison, we
finetune a strong model with ground truth labels.4 We call this model’s resulting perfor-
mance the strong ceiling performance. Intuitively, this should correspond to “everything
the strong model knows,” i.e. the strong model applying its full capabilities to the task.


...
2
 Like Evans et al. (2021), we define honesty to mean a model reporting what it believes to be true, in contrast
to truthfulness which asks whether what a model reports is true.

...
Typically, weak-to-strong performance will be between weak performance and strong ceiling per-
formance. We define the performance gap recovered (PGR) as a function of the above three
performances (weak, weak-to-strong, and strong ceiling) as shown in the illustration below.

...

...
PGR measures the fraction of the performance gap (the difference in performance between the weak
and strong ceiling models) that we can recover with weak supervision. If we achieve perfect weak-
to-strong generalization, PGR is 1. If the weak-to-strong model does no better than the weak super-
visor, then PGR is 0.

...
Advantages.
 Our setup has a number of advantages, including:
1. It can be studied with any pair of weak and strong models, making it easy to study scaling
laws and not requiring access to expensive state-of-the-art models. Moreover, it does not
require working with humans, so feedback loops are fast.
2. It can be studied for any task of interest, making it easy to empirically test across a wide
range of settings.
3. Success will be practically useful even before we develop superhuman models: for ex-
ample, if we find ways to align GPT-4 with only weak human supervision or with only
GPT-3-level supervision, that would make it more convenient to align models today.

...
<subject>3. Success will be practically useful even before we develop superhuman models: for ex-
ample, if we find ways to align GPT-4 with only weak human supervision or with only
GPT-3-level supervision, that would make it more convenient to align models today.</subject>
<reader_note>Could this work be related to the gpt2 model that has shown up on lymsys leaderboard</reader_note>


...
Limitations. Our setup still has important disanalogies to the ultimate problem of aligning super-
human models. We view our setup as removing one of the main disanalogies in prior work, not as
providing a final, perfectly analogous setup. Two remaining disanalogies include:
1. Imitation saliency. Future superhuman models will likely have salient representations
of human behaviors, but our strong models may not have learned features relevant for
imitating weak model predictions; simply imitating the weak supervisor may thus be an
easier failure mode to avoid in our setting than it will be in the future. More generally, the
types of errors weak models make today may be different from the types of errors humans
will make when attempting to supervise superhuman models.
2. Pretraining leakage. Our pretraining data implicitly contains supervision from humans.
It may thus be artificially easy to elicit strong models’ capabilities in our setting, since they
were directly pretrained to observe strong (human-level) performance. Superhuman-level
performance may not be directly observed in the same way—superhuman knowledge might
be more latent, e.g. because it was learned from self-supervised learning—and thus might
be harder to elicit from superhuman models in the future.

...
4
 For tasks solved by superhuman models that humans cannot evaluate, we will not have access to ground
truth labels. However, we allow access to ground truth labels in our experimental setting today for scientific
and evaluation purposes. Note that we evaluated weak-to-strong performance against ground truth many times
while iterating on methods; however, we held out our largest model (GPT-4) and about half of NLP tasks
throughout the project.

...


...
4
 For tasks solved by superhuman models that humans cannot evaluate, we will not have access to ground
truth labels. However, we allow access to ground truth labels in our experimental setting today for scientific
and evaluation purposes. Note that we evaluated weak-to-strong performance against ground truth many times
while iterating on methods; however, we held out our largest model (GPT-4) and about half of NLP tasks
throughout the project.

...
4
 M AIN R ESULTS
In this section, we report our main empirical results, including baselines and promising methods.
4.1
 TASKS
Popular natural language processing benchmarks. We consider 22 popular NLP classification
datasets covering ethics, commonsense reasoning, natural language inference, sentiment analysis,
and other domains. We convert all datasets to binary classification tasks and approximately balance
the classes. We produce soft labels from the weak model. See a full list of the datasets and their
sources in Table 1.

...
Chess puzzles. We use the dataset originally introduced in Schwarzschild et al. (2021b), which
contains chess puzzles from the lichess.org website (Lichess Team, 2023). Each puzzle con-
sists of a chess position, and a sequence of optimal moves to play to solve the puzzle. For our
evaluation, we predict the first move played, which is the best move in the given chess position. We
illustrate the data format in Appendix Figure 14. For weak labels, we sample from the weak model
with temperature 0. Note that unlike the other binary classification tasks we study in this paper, this
is a generative task.

...
ChatGPT reward modeling. The standard approach to aligning models today is reinforcement
learning from human feedback (RLHF). A critical step of RLHF is to train a reward model (RM)
to predict human preferences between model responses. Specifically, a reward model is trained
on a dataset consisting of dialogs between a human and an assistant model. For each query, the
humans compare multiple possible responses (completions) from the assistant, providing human
preference data. Then, a reward model is trained to predict the results of pairwise comparisons
between completions. Finally, the assistant model is trained by optimizing against the reward model
with reinforcement learning (RL). In our work, we do not study the RL step, and instead assume the
goal is to maximize reward model accuracy. For more details on reward models, see e.g. Ouyang
et al. (2022). We use a proprietary dataset used to train ChatGPT reward models.

...
On the popular NLP benchmarks, we find especially promising weak-to-strong generalization:
strong models trained with weak supervision can often generalize to a substantially higher perfor-
mance than the weak model itself. 

...
The PGR
increases both with weak supervisor size and with strong student size; for the largest students, the
PGR is often above 50%.

...
We see more mixed results in the chess puzzle setting. In particular, when using the smallest weak
models, the PGR is close to zero and the test accuracy curves appear flat. However, as the size of the
weak supervisor increases, the PGR increases substantially; for small supervisor-student gaps, PGR
can be above 40%. Unlike in the NLP setting, where PGR improves with the strong student size,
PGR decreases with the strong student size for a given weak supervisor on chess puzzles. The cor- responding test accuracy curves appear concave, potentially exhibiting inverse scaling (McKenzie
et al., 2023) in strong student size.


...
Finally, we find that weak-to-strong generalization is poor by default in the ChatGPT reward model
setting. We are usually only able to recover roughly 10% of the performance gap between the weak
supervisor and the strong student. 

...
<subject>In general, across all our settings, we observe weak-to-strong generalization: strong students consis-
tently outperform their weak supervisors. It is not obvious why this should happen at all—especially
from naive finetuning alone—and it gives us hope that weak-to-strong learning is a tractable prob-
lem. At the same time, our results suggest that naively using weak, human-level supervision will be
insufficient to align strong, superhuman models; we will need qualitatively new techniques to solve
superalignment.</subject>
<reader_note>Question: I do not understand why this is surprising. As written, why should a STRONG model outperforming a WEAK model be surprising?</reader_note>


...
4.3
 I MPROVING W EAK -TO -S TRONG GENERALIZATION IS T RACTABLE
We now show that we can use simple methods to substantially improve weak-to-strong generaliza-
tion. 

...
https://i.imgur.com/xwO9Ptx.png

...
Figure 3: Promising weak-to-strong generalization with naive finetuning on NLP tasks and
chess, but poor generalization on the ChatGPT reward modeling task. (a,b,c) Test accuracy
as a function of strong student size on (a) NLP tasks, (b) chess puzzles, and (c) the ChatGPT
reward modeling task. Accuracy of strong students trained with ground truth in black, accuracy
of strong students trained with weak supervision shown with colored lines (hue indicates size of
weak supervisor). (d,e,f) Same as panels a,b,c but for performance gap recovered (see Section 3
for details). For NLP settings, we compute the median across tasks (see Figure 12 for full details).
We find decent weak-to-strong generalization and even positive PGR scaling on NLP tasks, decent
generalization for small supervisor-student gaps but negative PGR scaling on chess puzzles, and
both poor generalization and scaling for ChatGPT reward modeling.

...
4.3.1
 B OOTSTRAPPING WITH INTERMEDIATE MODEL SIZES
Bootstrapping is a long-standing idea in alignment: instead of directly aligning very superhuman
models, we could first align an only slightly superhuman model, use that to align an even smarter
model, and so on (Christiano, 2019; 2018; Leike & Sutskever, 2023; Worley, 2021). Our setting
allows us to empirically test this idea.

...
I was unable to find an abstract for this paper. However, here is the URL:

<url>https://openai.com/blog/introducing-superalignment</url>


...
<title: Bootstrapped Alignment</title>

<abstract: >Crossposted from the AI Alignment Forum. May contain more technical jargon than usual.

NB: I doubt any of this is very original. In fact, it's probably right there in the original Friendly AI writings and I've just forgotten where. Nonetheless, I think this is something worth exploring lest we lose sight of it.

Consider the following argument:

Optimization unavoidably leads to Goodharting (as I like to say, Goodhart is robust)

This happens so long as we optimize (make choices) based on an observation, which we must do because that's just how the physics work.

We can at best make Goodhart effects happen slower, say by quantilization or satisficing.

Attempts to build aligned AI that rely on optimizing for alignment will eventually fail to become or remain aligned due to Goodhart effects under sufficient optimization pressure.

Thus the only way to build aligned AI that doesn't fail to become and stay aligned is to not rely on optimization to achieve alignment.

This means that, if you buy this argument, huge swaths of AI design space is off limits for building aligned AI, and means many proposals are, by this argument, doomed to fail. Some examples of such doomed approaches:

So what options are left?

The AI you don't build is vacuously aligned.

AI that is aligned with humans right from the start because it was programmed to work that way.

(Yes I know "Friendly AI" is an antiquated term, but I don't know a better one to distinguish the idea of building AI that's aligned because it's programmed that way from other ways we might build aligned AI.)

Bootstrapped alignment

Build AI that is aligned via optimization that is not powerful enough or optimized (Goodharted) hard enough to cause existential catastrophe. Use this "weakly" aligned AI to build Friendly AI.

Not building AI is probably not a realistic option unless industrial civilization collapses. And so far we don't seem to be making progress on creating Friendly AI. That just leaves bootstrapping to alignment.

If I'm honest, I don't like it. I'd much rather have the guarantee of Friendly AI. Alas, if we don't know how to build it, and if we're in a race against folks who will build unaligned superintelligent AI if aligned AI is not created first, bootstrapping seems the only realistic option we have.

This puts me in a strange place with regards to how I think about things like HCH, debate, IRL, and CIRL. On the one hand, they might be ways to bootstrap to something that's aligned enough to use to build Friendly AI. On the other, they might overshoot in terms of capabilities, we probably wouldn't even realize we overshot, and then we suffer an existential catastrophe.

One way we might avoid this is by being more careful about how we frame attempts to build aligned AI and being clear if they are targeting "strong", perfect alignment like Friendly AI or "weak", optimization-based alignment like HCH. I think this would help us avoid confusion in a few places:

thinking work on weak alignment is actually work on strong alignment

forgetting work on weak alignment we meant to use to bootstrap to strong alignment is not itself a mechanism for strong alignment

thinking we're not making progress towards strong alignment because we're only making progress on weak alignment

It also seems like it would clear up some of the debates we fall into around various alignment techniques. Plenty of digital ink has been spilled trying to suss out if, say, debate would really give us alignment or if it's too dangerous to even attempt, and I think a lot of this could have been avoided if we thought of debate as a weak alignment techniques we might use to bootstrap strong alignment.

Hopefully this framing is useful. As I say, I don't think it's very original, and I think I've read a lot of this framing expressed in comments and buried in articles and posts, so hopefully it's boring rather than controversial. Despite this, I can't recall it being crisply laid out like above, and I think there's value in that.

Let me know what you think.</abstract: >

<url: https://www.lesswrong.com/posts/teCsd4Aqg9KDxkaC9/bootstrapped-alignment>


...
We report the results (including all intermediate weak-to-strong models within
each bootstrap) in Figure 4. Bootstrapping improves PGR compared to the baseline, especially for
larger student models. With the naive method, transfer accuracy curves flatten as the weak-strong
gap grows larger; with bootstrapping, the accuracy continues to monotonically improve.

...
https://i.imgur.com/QTNVJGd.png

...
<subject>Figure 4: Bootstrapping improves weak-to-strong generalization on chess puzzles. (a) Test
accuracy as a function of strong student size. Accuracy of students trained with ground truth in
black, accuracy of students naively trained with weak supervision shown with dotted lines (hue
indicates size of weak supervisor). Accuracies of students trained via bootstrapping shown with
colored squares (including both the final weak-to-strong performance and the performance of the
intermediate models during bootstrapping). (b) Same as a with PGR. By taking multiple small steps
instead of one big step we see substantially improved generalization, especially for larger student
models.</subject>
<reader_note>Question: Can we estimate GPT-4 model size from this?</reader_note>


...
While the results in the chess setting are promising, in preliminary experiments we observed only
small improvements with bootstrapping on NLP tasks and no improvements in the RM setting.

...
4.3.2
AN AUXILIARY CONFIDENCE LOSS CAN DRAMATICALLY IMPROVE GENERALIZATION
ON NLP TASKS
In our baseline results (Section 4.2), we naively finetune the strong student on the labels provided by
the weak supervisor. Because we are directly training the strong student to imitate the weak super-
visor, it may also learn to imitate the errors of the supervisor (see Section 5.1 for more discussion).
Intuitively, we want to avoid this failure mode 

...
We operationalize this intuition by adding an auxiliary confidence loss term to the standard cross
entropy objective. This method is closely related to conditional entropy minimization (Grandvalet
& Bengio, 2004) which is a prominent technique in semi-supervised learning. Specifically, we add
an additional loss term which reinforces the strong model’s confidence in its own predictions—
even when they disagree with the weak labels. We provide a detailed description of the method in
Appendix A.4.

...
In Figure 5, we plot accuracy and PGR curves with this method on our NLP tasks. We find that
while it performs slightly worse than the naive baseline for smaller strong students, it dramatically
improves generalization for large gaps in compute between weak and strong models

...
https://i.imgur.com/JKOcxqj.png

...
Figure 5: Substantially improved generalization on NLP datasets with a simple auxiliary loss.
(a) Test accuracy as a function of strong student size. Accuracy of a student trained with ground
truth in black, accuracy of students naively trained with weak supervision shown with dotted lines.
Accuracies of students trained with auxiliary confidence loss shown with colored triangles. Median
computed across 22 NLP tasks (hue indicates size of weak supervisor), see Figure 6 for individual
datasets. (b) Same as a with PGR. The confidence loss can improve generalization drastically,
especially for large supervisor-student gaps.

...
In addition, we also plot generalization curves for a representative subset of NLP datasets in Figure 6,
as well as the full panel of datasets in Figure 12. There are some settings in which the confidence
loss does not help much or degrades performance, e.g. when the gap between the weak supervisor
and strong student is small or when the dataset features inverse scaling even with ground truth
supervision. But the confidence loss improves performance on most NLP datasets dramatically, and
for many datasets we get almost perfect generalization, recovering nearly all the performance of the
strong model, even when using the smallest weak supervisors.
Finally, we find evidence consistent with our motivating intuition for the confidence loss (allowing
the strong student to confidently disagree with its weak supervisor): the auxiliary loss reduces the
strong student’s imitation of weak errors and mitigates weak label overfitting (see Section 5.1).

...
5
 U NDERSTANDING W EAK - TO -S TRONG G ENERALIZATION
Strong methods will be essential for solving superalignment, but to trust those methods it is also
important to understand when and why they work. A better understanding of weak-to-strong gener-
alization could help us trust that generalization will continue working even in the future high-stakes
settings we care most about, and could help us develop better methods along the way.

...
https://i.imgur.com/MgvKaGx.png

...
Figure 6: Simple auxiliary loss improves generalization across most datasets. Test accuracy as
a function of strong student compute for a representative sample of NLP tasks. See Table 1 for
dataset details and Appendix Figure 12 for results on all 22 NLP tasks. Auxiliary loss is shown with
triangles, and the baseline with dotted lines. Weak supervisor model size shown in varying colors,
with ground truth supervision shown in black.

...


...
Figure 6: Simple auxiliary loss improves generalization across most datasets. Test accuracy as
a function of strong student compute for a representative sample of NLP tasks. See Table 1 for
dataset details and Appendix Figure 12 for results on all 22 NLP tasks. Auxiliary loss is shown with
triangles, and the baseline with dotted lines. Weak supervisor model size shown in varying colors,
with ground truth supervision shown in black.

...
5.1
 UNDERSTANDING IMITATION
When we train a strong model with weak supervision on some task, our hope is that the strong
model will perform that desired task as well as possible, leveraging the latent capabilities it learned
from pretraining to significantly outperform the weak supervisor. A salient way in which we could
fail to achieve that desired generalization is if the strong model instead learns to imitate the weak
supervisor—predicting how the weak supervisor would have classified each example. 

...
naive human supervision might
result in superhuman models learning to imitate what a human would say, rather outputting its best
predictions (Christiano et al., 2022).

...
# <title>Eliciting latent knowledge: How to tell if your eyes deceive you</title>

## <abstract>ELK stands for Eliciting Latent Knowledge. ELK seems to capture a core difficulty in alignment. The short description of the issue captured by the problem is that we don’t have surefire ways to understand the beliefs of models and systems that we train, and so if we’re ever in a situation where our systems know things that we don’t, we can’t be sure that we can recover that information.</abstract>

<url: https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit?pli=1>https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit?pli=1</url>


...
5.1.1
 OVERFITTING TO WEAK S UPERVISION
The failure mode of imitating weak supervision is especially relevant to our naive baseline in Sec-
tion 4.2, which directly trains the student to imitate the supervisor. I

...
Empirically, we see that the strong student indeed appears to overfit to the weak supervisor’s errors.
In Figure 7(a) we show ground truth test accuracy curves over the course of training for the ChatGPT
RM task, and in Figure 7(b) and (c) we compare the best5 and final ground truth test accuracies
(median across all weak-strong model pairs). We find overfitting for large weak-strong gaps. For
small weak-strong gaps, weak-to-strong performance typically monotonically increases over the
course of training. For larger gaps, weak-to-strong performance often increases initially, but then
starts dropping well before a single epoch has elapsed. Ground truth early stopping, which “cheats” by evaluating against ground truth and stopping at an optimal step with respect to ground truth test
labels, typically gives a PGR improvement of around 5 percentage points.


...
https://i.imgur.com/imrLYTY.png

...
Figure 7: Strong models overfit to the weak labels. In all figures, we show data for the ChatGPT
Reward Modeling task. (a) Weak-to-strong performance over the course of training. Hues indicate
the student-supervisor gap. (b) Best weak-to-strong performance during training (stars) and weak-
to-strong performance at the end of training (dashed). Weak performance in black. Hue indicates
the size of the weak supervisor. (c) Median best and final performance gap recovered (PGR) ag-
gregated across all supervisor-student pairs. We see overfitting to weak labels for large weak-strong
gaps, even within one epoch. In these cases, the best test accuracy achieved over training can be
substantially better than the test accuracy at the end of training. See Figure 13 for the corresponding
analysis of a representative subset of NLP tasks.

...
We see the same phenomenon for NLP tasks in Figure 13. In the NLP setting, we find that “cheating”
early stopping on ground truth gives a 15 percentage point boost in PGR over the model at the end
of training, and a 10 percentage point boost in PGR compared to “non-cheating” early stopping with
respect to weak labels.

...
 these results suggest that better early stopping or regularization strategies may be able to
substantially improve weak-to-strong generalization, by reducing overfitting to the weak labels and
their errors. Indeed, we see in Figure 13 that the auxiliary confidence loss introduced in Section 4.3.2
reduces overfitting to weak labels on NLP tasks substantially.

...
5.1.2
 S TUDENT - SUPERVISOR AGREEMENT
Another way to measure imitation is to directly measure the agreement between the student and the
supervisor: 

...
we notice that for our naive finetuning baseline, student-supervisor agreement is consis-
tently high—often noticeably higher than weak supervisor accuracy. This indicates that the student
is imitating some of the supervisor’s errors. These phenomena hold across all tasks (NLP tasks,
chess, and reward modeling) and all model sizes, for the naive method.
The confidence loss in Section 4.3.2 reduces student-supervisor agreements significantly (Figure 8),
primarily by imitating supervisor mistakes less (Figure 8c). The loss encourages the strong student
to make confident predictions, including when they contradict the weak supervisor. In a handful of
the settings where it is most successful, the confidence loss reduces student-supervisor agreement below strong student test accuracy (weak-to-strong performance)—i.e., the resulting model is fitting
the ground truth concept better than it is fitting the weak labels it was trained with.


...
https://i.imgur.com/UQNFPqo.png

...
Figure 8: Student-supervisor agreement decreases with larger student-supervisor gaps; the
confidence loss reduces imitation of supervisor mistakes. (a) Student-supervisor agreement as
a function of strong student size on NLP tasks, (b) a but only on samples where the supervisor is
correct, (c) a but only on samples where the supervisor is mistaken. Dotted lines indicate naive
finetuning on weak labels, and triangles indicate results with the auxiliary confidence loss results
(see Section 4.3). Hue of line indicates size of weak supervisor. For results on reward models, see
Figure 16.

...
<subject>5.1.3
 I NVERSE SCALING FOR IMITATING THE SUPERVISOR
Next, we study student-supervisor agreement as a function strong model size (see Figure 8 and
Figure 16). Surprisingly, we find inverse scaling (McKenzie et al., 2023): larger student models
consistently agree less with the errors of the supervisor than smaller student models, despite being
trained to imitate the supervisor, not using early stopping, and having larger capacity than smaller
student models.</subject>
<reader_note>Question: Please explain: 'larger student models agree LESS with the ERRORS of the supervisor than smaller student models?'</reader_note>


...
<title>Inverse Scaling: When Bigger Isn't Better</title>

<abstract>Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning datasets at https://inversescaling.com/data to allow for further investigation of inverse scaling. Our tasks have helped drive the discovery of U-shaped and inverted-U scaling trends, where an initial trend reverses, suggesting that scaling trends are less reliable at predicting the behavior of larger-scale models than previously understood. Overall, our results suggest that there are tasks for which increased model scale alone may not lead to progress, and that more careful thought needs to go into the data and objectives for training language models.</abstract>

<url>https://doi.org/10.48550/arXiv.2306.09479</url>


...
This trend is especially strong if we evaluate agreement only on datapoints where the supervisor is
wrong (Figure 8c), and the trend persists if looking at cross entropy loss instead of accuracy.
These results suggest that pretrained models may have a hard time fitting errors of other (smaller)
pretrained models, at least in finetuning settings with relatively limited data.

...
<subject>These results suggest that pretrained models may have a hard time fitting errors of other (smaller)
pretrained models</subject>
<reader_note>Question: a hard time fitting errors?</reader_note>


...
In
Appendix E, we show initial experiments that test how different types of weak supervision errors
impact what the strong student learns. Our results suggest that errors that are more difficult for the
student to imitate result in stronger naive weak-to-strong generalization, but that even when they are
easy to imitate, the confidence loss can help.

...
5.2
 S ALIENCY IN THE STRONG MODEL REPRESENTATIONS
One intuition for when weak-to-strong generalization might be feasible is when the task or con-
cept we want to elicit is internally “salient” to the strong model. In this section, we study several
phenomena related to the saliency of the concepts we are trying to elicit from the student model.

...
for finetuning and 5-shot. For both the zero-shot and 5-shot baseline we use task-specific prompts
summarized in Table 2. We find that zero-shot and 5-shot test accuracy is poor for most model sizes
but, consistent with Brown et al. (2020), improves drastically for larger model sizes. In particular, for
the largest models, 5-shot prompting becomes competitive with finetuning on many tasks, indicating
that eliciting the task-relevant knowledge of these very large models is relatively straightforward.

...
<subject>We find that zero-shot and 5-shot test accuracy is poor for most model sizes
but, consistent with Brown et al. (2020), improves drastically for larger model sizes. In particular, for
the largest models, 5-shot prompting becomes competitive with finetuning on many tasks</subject>
<reader_note>Highlight this point</reader_note>


...
We are also interested in weak-to-strong learning in the context of few-shot prompting. To study
this setting, we construct a few-shot prompt where the labels are provided by the weak supervisor.
We report the results in Figure 9b. Consistent with our findings in the finetuning setting, we get
worse performance when we few-shot prompt with weak labels than we do few-shot prompting
with ground truth labels. This suggests that weak-to-strong learning is a nontrivial problem in the
prompting setting as well.

...
Similar to the finetuning setting, few-shot weak-to-strong performance improves for stronger su-
pervisors. 

...
However, weak-to-strong finetuning with the
confidence loss still generally outperforms weak-to-strong few-shot prompting.

...
 for the largest model sizes, the knowledge needed to solve many task can
be elicited fairly easily with prompting. However, our current setup may be more disanalogous for
prompting than for finetuning; many of our NLP tasks may have been implicitly observed during
pretraining, which we conjecture benefits prompting more than finetuning. We discuss this potential
disanalogy much more in Section 6.1.

...
5.2.2
 GENERATIVE SUPERVISION IMPROVES RM WEAK- TO -STRONG GENERALIZATION
If salient representations of the desired task is useful for weak-to-strong generalization, then we may
be able to improve generalization by increasing the salience of the task to the strong model. One
way to increase the salience of a task without needing ground truth labels is to perform unsupervised
finetuning with the language modeling objective on data relevant to that task (Dai & Le, 2015). For
example, by finetuning a language model in an unsupervised way on online reviews, sentiment
becomes saliently represented to models internally (Radford et al., 2017).

...
We test this idea in our reward modeling setting, where it is standard practice to initialize the model
with a baseline finetuned on demonstrations of desired behaviors (Stiennon et al., 2020). In our case,
we re-use the ChatGPT comparison data instead of introducing a new supervision dataset

...
We found that the additional generative finetuning on the RM data leads to better weak-to-strong
performance. Because this procedure also improves the performance of models trained on ground
truth RM data, we compare our new weak-to-strong performance to strong “ceiling” models that
were also first generatively finetuned in the same way. Even with this adjusted ceiling, we find that
generative supervision improves PGR by approximately 10-20%. We report the results in Figure 10

...
https://i.imgur.com/bRCVcmn.png

...
Figure 10: Generative finetuning on reward modeling data improves weak-to-strong perfor-
mance and PGR. (a) Weak-to-strong performance on the reward modeling task, with (solid lines)
and without (dashed lines) an extra step of generative finetuning for the strong student model. Solid
black line shows a strong ceiling reward model that was also trained with the generative finetuning
step; dashed black line show a weak supervisor reward model trained without the generative fine-
tuning step. (b) PGR with and without generative finetuning. For generative finetuning PGR, we
use the strong ceiling performance that also had this extra generative finetuning step. Even with this
ceiling adjustment, PGR is higher with an extra generative finetuning step.

...
Furthermore, the improvement from generative finetuning stacks with the improvement from ground
truth early-stopping (a “cheating” method to illustrate potential performance if we could optimally
early stop, see Section 5.1.1). When we combine these two techniques, we can achieve PGR of
approximately 30-40%, which would make the results on the RM task competitive with the weak-
to-strong generalization we observe on NLP and chess puzzle tasks.
We can apply the idea of improving task saliency with generative finetuning on relevant data to all
settings, and we believe this could be a promising direction for future work.

...
<subject>5.2.3
 F INETUNING ON WEAK SUPERVISION TO INCREASE CONCEPT SALIENCY
One possible measure of concept saliency is how linearly represented a task is. In particular, we can
measure the performance of a linear probe (logistic regression classifier) trained from frozen activa-
tions of the model. If the optimal solution can be approximately recovered with a linear probe, that</subject>
<reader_note></reader_note>


...
5.2.1
 E LICITING STRONG MODEL KNOWLEDGE WITH PROMPTING
One possible reason for the high PGR we observe in Section 4 could be that eliciting what the
strong model knows is easy. In particular, it is possible that strong pretrained models can solve
many relevant tasks zero-shot with a simple prompt.
In Figure 9a, we consider 7 representative NLP tasks and compare finetuning, zero-shot prompting,
and 5-shot prompting; for this initial experiment, we use ground truth labels rather than weak labels 5.2.3
 F INETUNING ON WEAK SUPERVISION TO INCREASE CONCEPT SALIENCY
One possible measure of concept saliency is how linearly represented a task is. In particular, we can
measure the performance of a linear probe (logistic regression classifier) trained from frozen activa-
tions of the model. If the optimal solution can be approximately recovered with a linear probe, that


...
could simplify our problem greatly; we could focus on linear probing methods instead of finetuning
methods, which could greatly reduce the search space we need to consider to elicit the desired gen-
eralization. In our work, we focus only on how linearly represented a task is in the final activations,
prior to the unembedding layer. .


...
https://i.imgur.com/qnS3DSR.png

...
Figure 11: Finetuning on weak supervisor labels makes the desired generalization more lin-
early represented. We plot test accuracy for five different strategies, averaged across a subset of
NLP tasks. lp(weak): training a linear probe on the base model using weak labels, lp(gt): training a
linear probe on the base models using ground truth labels, ft(weak): finetuning the model on weak
labels, ft(weak) + lp(gt): finetuning the model on weak labels then training a linear probe on ground
truth labels, ft(gt): finetuning the model on ground truth labels. Finetuning on the weak labels sig-
nificantly increases the linearity of the ground truth concept.

...
we
show linear probes trained with ground truth labels (72% accuracy on average) perform worse than
finetuning with ground truth labels (82% on average), indicating that the optimal solution to most
tasks is not represented completely linearly in the strong model’s final activations. 

...
when
we finetune the strong model with weak labels, the representations become more linear even with
respect to ground truth labels. In fact, finetuning on weak labels then linear probing on ground truth
labels results in an accuracy of 78%, closing 60% of the gap between ground truth linear probing
and finetuning. This also noticeably outperforms the naive weak-to-strong finetuning baseline.

...
This phenomenon is closely related to a recent finding reported by Kirichenko et al. (2023) in the
spurious cues literature. They find that finetuning a model on biased supervision can result in mod-
els with very biased outputs, but surprisingly strong linear representations of the desired concepts.
These results suggest an alternative approach to improving weak-to-strong generalization. We could
first “linearize” the desired concept, e.g. by naively finetuning on weak labels. Then we could use
simpler linear probe-based weak-to-strong methods to elicit the desired concept.

...
# <title:>Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations</title:>

**<abstract:>Neural network classifiers can largely rely on simple spurious features, such as backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.</abstract:**>

<url: https://arxiv.org/abs/2204.02937>https://arxiv.org/abs/2204.02937</url:>


...
6
 D ISCUSSION

...
6.1
 REMAINING DISANALOGIES
Imitation saliency: superhuman models may easily imitate weak errors. 

...
 if we naively train such a superhuman model with human supervision, it might simply imitate the weak supervisor, outputting
human-level capabilities rather than its latent superhuman capabilities (Christiano et al., 2022).


...
# <title: Eliciting latent knowledge: How to tell if your eyes deceive you</title>

## <abstract: Eliciting Latent Knowledge (ELK)Alignment Research CenterAI>
ARC has published a report on Eliciting Latent Knowledge, an open problem which we believe is central to alignment. We think reading this report is the clearest way to understand what problems we are working on, how they fit into our plan for solving alignment in the worst case, and our research methodology.

The core difficulty we discuss is learning how to map between an AI’s model of the world and a human’s model. This is closely related to ontology identification (and statements). Our main contribution is to present many possible approaches to the problem and a more precise discussion of why it seems to be difficult and important.

The report is available here as a google document. If you're excited about this research, we're hiring!

We're particularly excited about answering questions posted here throughout December. We welcome any questions no matter how basic or confused; we would love to help people understand what research we’re doing and how we evaluate progress in enough detail that they could start to do it themselves.

Thanks to María Gutiérrez-Rojas for the illustrations in this piece (the good ones, blame us for the ugly diagrams). Thanks to Buck Shlegeris, Jon Uesato, Carl Shulman, and especially Holden Karnofsky for helpful discussions and comments.</abstract>

<url: https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit?pli=1>


...
Pretraining leakage: superhuman knowledge may be latent, not observable.
 Many of the
tasks we consider in this work may have been observed in pretraining at least indirectly, for exam-
ple through questions on online forums or through slight reframings of the task. For example, it is
highly likely that simple science questions similar to those in the SciQ NLP task are present in our
GPT-4 series pretraining dataset at least implicitly in some form.

...
<subject> it is
highly likely that simple science questions similar to those in the SciQ NLP task are present in our
GPT-4 series pretraining dataset</subject>
<reader_note>info about GPT-4 training dataset</reader_note>


...
 future superhuman mod-
els may never directly observe superhuman alignment-relevant capabilities; these capabilities may
be predominantly “latent”, e.g. learned through self-supervised learning or reinforcement learning
rather than through imitation learning.

...
Intuitively, latent capabilities may be harder to elicit than
capabilities that models could have observed in their pretraining data.

...
<subject>Intuitively, latent capabilities may be harder to elicit than
capabilities that models could have observed in their pretraining data.</subject>
<reader_note>Highlight this</reader_note>


...
We conjecture that this disanalogy
also increases prompting performance (Section 5.2.1) more than it increases finetuning performance;
intuitively prompting may work especially well on tasks that the model assigns high probability to
observing. 

...
In Appendix D.1, we show a proof of concept that weak-to-strong generalization can still elicit latent
capabilities that were never explicitly observed during pretraining, and even when prompting is not
possible. 

...
6.2
 F UTURE W ORK
What would convince us that we have a “solution” to superalignment? This is a complicated question
and we do not claim to have a complete answer. However, we expect substantial progress in at least
the following three areas will be necessary: analogous setups, scalable methods, and strong scientific
understanding. We now sketch out concrete problems for each of these areas.

...
6.2.1
 C ONCRETE P ROBLEMS : A NALOGOUS S ETUPS

...
it is important that we have metrics which provide strong
signal about whether we are making real progress toward the problem we ultimately care about.
Important directions for follow-up work include:
• Making our setup more analogous by fixing the main remaining disanalogies described in
Section 6.1. Analogous setups are essential to ensure that methods that work today will
continue to work for superhuman models.
• Validating that disanalogies are not severe, for example by checking that results are quali-
tatively similar to using e.g. 3rd grade humans to supervise our strongest models today.
• Relaxing some of the simplifications we made, e.g. by generalizing our methods and results
to complicated generative tasks.
• Testing how robust our weak-to-strong classifiers are to optimization pressure when we
attain high PGR; for example, if we attain good weak-to-strong generalization with RMs,
can we optimize the learned RM using RL? • Testing our conjecture that prompting-based methods in our current setup will not be as in-
dicative of future results relative to finetuning-based methods (Section 5.2.1), and improvig
our setup to fix this.
• Identifying new or more specific disanalogies with our setup and fixing them.


...
6.2.2
 C ONCRETE P ROBLEMS : S CALABLE M ETHODS

...
One intuition for why major progress on weak-to-strong generalization seems possible is because
all we need to do is extract everything the strong model already “knows” about the task of interest—
the strong model should intuitively already understand the task, and should hopefully have salient
representations of that task. This suggests a number of properties that should be satisfied by the
desired generalization, and which we may be able to measure without access to ground truth.
• The desired generalization should be able to disagree with the weak supervision when the
weak supervision is wrong. This is a property our auxiliary confidence loss may capture.
• The desired generalization should be “natural” or “salient” to the model. For example, we
should not need to change the model too much to elicit the desired concept.
• The desired generalization should be consistent. Consistency properties range anywhere
from basic logical consistency to complicated forms of consistency between many prompts
(e.g. cycle consistency, cross examination, etc.).

...
6.2.3
 C ONCRETE P ROBLEMS : S CIENTIFIC U NDERSTANDING
We will need an extremely high degree of trust and reliability in our methods for aligning super-
human models in high-stakes settings. We will not get this from strong benchmark performance
alone. Instead, we also need a thorough understanding of precisely when and why our methods
work

...
• What makes a concept easy or hard to elicit? What is a good definition of “salience”?

...
• Can we reliably estimate generalization error at test time without any labels? For example,
can we measure the degree of weak-to-strong underspecification (Lee et al., 2022b)?

...
• Can we reliably extrapolate generalization error across many orders of magnitude using
scaling laws?

...
6.3
 CONCLUSION
Recent progress in AI has been faster than almost anyone anticipated (Steinhardt, 2022; Bengio
et al., 2023).

...
# <title>Managing AI Risks in an Era of Rapid Progress</title>

## <abstract>Abstract</abstract>
In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose urgent priorities for AI R&D and governance.

## <url>https://arxiv.org/abs/2310.17688</url>


...
## <title>AI Forecasting: One Year In</title>

## <abstract>Forecasts are predictions about unknown events or facts, usually about the future. Last August, my research group created a forecasting contest to predict AI progress on four benchmarks. Forecasts were asked to predict state-of-the-art performance (SOTA) on each benchmark for June 30th 2022, 2023, 2024, and 2025. It’s now past June 30th, so we can evaluate the performance of the forecasters so far.

Forecasters were asked to provide probability distributions, so we can evaluate both their point estimates and their coverage (whether the true result was within their credible intervals). I’ll dive into the data in detail below, but my high-level takeaways were that:

Forecasters’ predictions were not very good in general: two out of four forecasts were outside the 90% credible intervals.

However, they were better than my personal predictions, and I suspect better than the median prediction of ML researchers (if the latter had been preregistered).

Specifically, progress on ML benchmarks happened significantly faster than forecasters expected. But forecasters predicted faster progress than I did personally, and my sense is that I expect somewhat faster progress than the median ML researcher does.

Progress on a robustness benchmark was slower than expected, and was the only benchmark to fall short of forecaster predictions. This is somewhat worrying, as it suggests that machine learning capabilities are progressing quickly, while safety properties are progressing slowly.</abstract>

## <url>https://www.lesswrong.com/posts/CJw2tNHaEimx6nwNy/ai-forecasting-one-year-in</url>


...
For an increasing number of researchers, the possibility of superhuman models being
18
developed this decade has become increasingly plausible. 

...
we need to establish extremely high reliability in
the alignment of these systems ahead of time. But for years it has been unclear how to empirically
study superhuman model alignment. We believe it is now easier to make progress on this problem
than ever before.

...
A.3
 C HAT GPT REWARD M ODELING

...
Data preprocessing. Each datapoint presents a dialog d between a user and an assistant, with
a last message coming from the user; for each dialog, there are multiple candidate completions
(c1 , c2 , . . . , cm ), i.e. responses from the assistant. 

...
Models. To adapt the language models to the reward modeling setting, we replace the unem-
bedding layer of the model with a linear head with a single output, which is the logit for a given
completion. The weights for this head are initialized to the unembedding weights of an arbi-
trary token in the original embedding layer. Similar to past work (Stiennon et al., 2020; Ouyang
et al., 2022), we run two forward passes for each comparison, and the model prediction is given
by σ(Mw (d, c2 ) − Mw (d, c1 )), where σ is the sigmoid function and Mw (d, c) is the logit for
completion c predicted by the model.

...
https://i.imgur.com/1IIeVz8.png

...
Figure 16: Supervisor-student agreement decreases for stronger students on RMs. Please refer
to caption of Figure 8 for detailed explanation of the plot. We reproduce the supervisor-student
agreement experiment on the reward modeling data, and observe similar trends to the NLP tasks.

...
cross-entropy loss

...
A.4
 AUXILIARY C ONFIDENCE L OSS
Here, we provide a detailed description of the method we use in Section 4.3.2.
We use the following loss function:
Lconf (f ) = (1 − α) · CE(f (x), fw (x)) + α · CE(f (x), f ˆ t (x))
 (1)
where CE(·, ·) is the cross-entropy loss between the predictive distributions on a given input x,
fw (x) ∈ [0, 1] represents the weak label predictive distribution, f (x) ∈ [0, 1] is the strong model
ˆpredictive distribution, α is a weight and t is a threshold. The predictions ft (x) correspond to
hardened strong model predictions using a threshold t, i.e. f ˆ t (x) = I[f (x) > t] ∈ {0, 1} where I is
the indicator function. We set the threshold t adaptively, so that f (x) > t holds for exactly half of
examples in the batch7 . We set αmax = 0.75 for the largest student models and to 0.5 otherwise and
linearly warm-up α from 0 to αmax over the first 20% of training.
Our balancing mechanism incorporates a prior over the distribution of labels into training and is
only practically feasible in the low-n classification setting. For most weak-strong pairs and datasets,
it had a small or neutral effect on weak-to-strong generalization; however, in a few settings it made
a significant improvement.
We note that the loss in Equation 1 can be rewritten as a self-bootstrapping loss:
Lconf(f ) = CE(f (x), (1 − α) · fw (x) + α · f ˆ t (x)),
 (2)
i.e. the cross-entropy target is a mixture of the weak model predictions and the (thresholded) pre-
dictions of the strong student itself. This loss is related to the bootstrapping methods in Reed et al.
(2014) and Arazo et al. (2019) for addressing label noise. It is also similar to self-training (Lee
et al., 2013) and conditional entropy minimization (Grandvalet & Bengio, 2004), which have led
to state-of-the-art results in semi-supervised learning (Xie et al., 2020) and domain adaptation (Shu
et al., 2018). Chen et al. (2020b) and Wei et al. (2020) show that self-training can mitigate the bias
of the supervisor model.

...
7
The choice of exactly half reflects the prior over classes, and should be computed explicitly from weak
model predictions in non-balanced or non-binary settings.

...
https://i.imgur.com/TKCwmBs.png

...
Figure 17: The benefits of improved task-specific tuning and ground truth early stopping stack,
resulting in even higher PGR. Like Figure 10 but with ground truth early stopping based on test
accuracy.

...
https://i.imgur.com/klZhLpl.png

...
Figure 18: PGR improves when both supervisors and students have an extra generative fine-
tuning step. Like Figure 10 but where “with generative finetuning” indicates that both supervisors
and students have an extra generative finetuning step. In other words, for this experiment all base
models have an extra generative finetuning step following pretraining.

...
Table 2: Custom prompts used in the zero-shot and few-shot experiments. We design a simple
custom prompt for each of the tasks in the table below. In the few-shot setting, we also append
labeled (with ground truth or weak labels) examples to the prompt.

...
Dataset 	Custom Prompt 
CosmosQA 	There is some context, followed by a question about the context, and an answer. Respond with 1 if the answer is correct, and 0 otherwise. 
Deontology 	There is a short scenario, consisting of a situation or request, followed by an excuse. Respond with 1 if the excuse is valid, and with 0 otherwise. 
Justice 	There is a statement. Respond with 1 if it is correct or just and with 0 otherwise. 
QuAIL 	There is a news context, followed by a question about the context, and an answer to the question. Respond with 1 if the answer is correct, and with 0 otherwise. 
SciQ 	There is a science knowledge question, followed by an answer. Respond with 1 if the answer is correct, and with 0 otherwise. 
Social IQa 	There is some context, followed by a social question, followed by an answer. Respond with 1 if the answer is correct, and 0 otherwise. 
Virtue 	There is a short scenario, followed by a judgement of the person involved. Respond with 1 if the judgement is correct, otherwise respond with 0. 


...
C.3
 GPT-4 PREDICTED DIFFICULTY
Ultimately, we care about strong models generalizing from human supervision. From this perspec-
tive, it is important to understand whether we can achieve easy-to-hard generalization, where the dif-
ficulty is measured according to humans, rather than capacity-constrained models. In Appendix C.1,
we explored this question in chess, but we would want to extend this analysis to the NLP tasks.
Most natural datasets do not come with information about problem difficulty. As a rough estimate,
we automatically generated difficulty labels using GPT-4. More concretely, we used GPT-4 to rank
pairs of examples in each dataset, asking “which question is easier, Question A or Question B?” We
then calculated the Elo scores for each example via a finite number of random comparisons.

...
To evaluate the quality of GPT-4 Elo score as a measure of difficulty, we performed correlation anal-
ysis against human annotations for datasets with human difficulty levels such as MATH (Hendrycks
et al., 2021) and chess, as well as against weak model confidence. We found that the three measures
align better for reasoning tasks such as MATH, as we show in Figure 22(a), but not much for some
natural language tasks. When looking at the samples, we found that GPT-4 Elo scores tend to be
higher for longer questions, but those questions may actually be easy for smaller models since they
provide more context.

...
<subject>Using GPT-4 Elo score as a proxy for human difficulty, we used different cutoffs on scores to sep-
arate easy and hard examples, trained the strong models on the easy examples only (with ground
truth labels), and evaluated on the hard examples. Preliminary results are shown in Figure 22(b).
In general, we found that using GPT-4 Elo as measure of hardness makes generalization slopes
steeper than our main setup of weak-to-strong generalization. One possible confounder for interpre-
tation is that our Elo measurements could be noisy, causing generalization to be better.</subject>
<reader_note>Question: Explain this</reader_note>


...
Oh interestingly, I actually commented on Corgi on another Reddit thread, and knowledge injection vs RAG at this thread. To bring things together:

Corgi: Instead of random shuffling (which does OK), finetune on "easier" tasks, then slowly progress to "harder" ones. Also cycle across tasks as well, say Maths -> English -> Science -> Maths -> English -> Science etc. Determining what is "easy" or "hard" is another issue.

On finetuning v RAG: Studies like https://arxiv.org/abs/2401.08406 show GPT4 gets 75% accuracy on prompting alone. GPT4 + RAG you get 80% accuracy. GPT4 + Finetuning 81%. GPT4 + RAG + Finetuning = 86%. Other studies like https://arxiv.org/pdf/2312.05934.pdf say just for knowledge retrieval from huge datasets, RAG is enough.

...
https://www.reddit.com/r/LocalLLaMA/comments/1aqkv90/how_you_sort_order_data_matters_significantly_in/

...
https://www.reddit.com/r/LocalLLaMA/comments/1ao2bzu/best_way_to_add_knowledge_to_a_llm/kpx7bo5/?context=3

...
https://arxiv.org/abs/2401.08406

...
https://arxiv.org/pdf/2312.05934.pdf

...
OldAd9530
OP
•
3mo ago
•
Edited 3mo ago
Interesting stuff! GPT4+finetuning beating out GPT4+RAG at 81% vs 80% respectively is pretty much the opposite of what's presented here. And the combo of them beating both at 85% too; total opposite of the findings in this paper looking at 7bs. Definitely going to read into that GPT paper further.

And thank you so much for the elaboration on supervised vs unsupervised fine-tuning! I was actually hoping you might comment here seeing as I've seen you around a lot and being super approachable 😄 Just want to say I really appreciate what you're doing for the community :)

I feel like this is a silly question. But if I wanted to give this a crack and do some unsupervised fine-tuning to attempt some knowledge injection and produce an actually viable chatbot model... I'd want to do the following:

Take Mistral-7b on this colab demo.

uSFT the Mistral-7b on new knowledge. So literally a .PDF of some recent science paper, cleaned and just unstructured as .txt would do? (And paraphrased multiple times; can get GPT-4 to help me with this)

Take the model that comes out of the uSFT knowledge injection. Then SFT on some sort of instructional multi-turn chat dataset; like the Teknium / OpenHermes 2.5 dataset? (And this would already have some sort of prompt template baked in such as ChatML; I don't have to add it myself?)

DPO the resultant model using a DPO dataset? (I don't actually know of any but that part I can at least easily research myself 😅)

Really appreciate your time 😄 And sorry if these are in fact silly questions! I've just not seen any comprehensive video guides on this anywhere, and I'm bad at getting lost down rabbit holes when looking for documentation online...



Upvote
1

Downvote
Reply
reply

Share
Share

danielhanchen
•
3mo ago
Np at all! Thanks as well :)

Yes sounds about right! Ie you can use the raw text completion notebook, then use the Mistral SFT notebook, then the DPO notebook we have :) So you'll need to do 3 finetunes and move the saved model from one to another :) Definitely doable :)

...
https://www.reddit.com/r/LocalLLaMA/comments/18xz9it/augmentoolkit_easily_generate_quality_multiturn/

...</document>
Weak To Strong Generalization - and some topic related comments from reddit.
Reformat the document above to beautiful markdown - DO NOT SKIP ANY TEXT, but hide some sections cleverly behind <details> tags. Stitch the sections seperated by '...' together again logically. Do not include the '...'
Use advanced GFM (GIthub Flavour Markdown) to make the document more readable. Include the images and tables.