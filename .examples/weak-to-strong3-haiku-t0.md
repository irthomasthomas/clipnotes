# Weak-to-Strong Generalization: Eliciting Capabilities of Superhuman Models

## Table of Contents
- [Weak-to-Strong Generalization: Eliciting Capabilities of Superhuman Models](#weak-to-strong-generalization-eliciting-capabilities-of-superhuman-models)
  - [Table of Contents](#table-of-contents)
  - [Abstract](#abstract)
  - [1. Introduction](#1-introduction)
  - [2. Related Work](#2-related-work)
    - [Learning from Noisy Labels with Deep Neural Networks: A Survey](#learning-from-noisy-labels-with-deep-neural-networks-a-survey)
    - [Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution](#fine-tuning-can-distort-pretrained-features-and-underperform-out-of-distribution)
    - [Exploring The Landscape of Distributional Robustness for Question Answering Models](#exploring-the-landscape-of-distributional-robustness-for-question-answering-models)
    - [The case for aligning narrowly superhuman models](#the-case-for-aligning-narrowly-superhuman-models)
    - [Artificial Sandwiching: When can we test scalable alignment protocols without humans](#artificial-sandwiching-when-can-we-test-scalable-alignment-protocols-without-humans)
  - [3. Methodology](#3-methodology)
  - [4. Main Results](#4-main-results)
    - [4.1 Tasks](#41-tasks)
    - [4.2 Baseline Results](#42-baseline-results)
    - [4.3 Improving Weak-to-Strong Generalization is Tractable](#43-improving-weak-to-strong-generalization-is-tractable)
      - [4.3.1 Bootstrapping with Intermediate Model Sizes](#431-bootstrapping-with-intermediate-model-sizes)
      - [4.3.2 An Auxiliary Confidence Loss Can Dramatically Improve Generalization on NLP Tasks](#432-an-auxiliary-confidence-loss-can-dramatically-improve-generalization-on-nlp-tasks)
  - [5. Understanding Weak-to-Strong Generalization](#5-understanding-weak-to-strong-generalization)
    - [5.1 Understanding Imitation](#51-understanding-imitation)
      - [5.1.1 Overfitting to Weak Supervision](#511-overfitting-to-weak-supervision)
      - [5.1.2 Student-Supervisor Agreement](#512-student-supervisor-agreement)
      - [5.1.3 Inverse Scaling for Imitating the Supervisor](#513-inverse-scaling-for-imitating-the-supervisor)
    - [5.2 Saliency in the Strong Model Representations](#52-saliency-in-the-strong-model-representations)
      - [5.2.1 Eliciting Strong Model Knowledge with Prompting](#521-eliciting-strong-model-knowledge-with-prompting)
      - [5.2.2 Generative Supervision Improves RM Weak-to-Strong Generalization](#522-generative-supervision-improves-rm-weak-to-strong-generalization)
      - [5.2.3 Finetuning on Weak Supervision to Increase Concept Saliency](#523-finetuning-on-weak-supervision-to-increase-concept-saliency)
  - [6. Discussion](#6-discussion)
    - [6.1 Remaining Disanalogies](#61-remaining-disanalogies)
    - [6.2 Future Work](#62-future-work)
      - [6.2.1 Concrete Problems: Analogous Setups](#621-concrete-problems-analogous-setups)
      - [6.2.2 Concrete Problems: Scalable Methods](#622-concrete-problems-scalable-methods)
      - [6.2.3 Concrete Problems: Scientific Understanding](#623-concrete-problems-scientific-understanding)
    - [6.3 Conclusion](#63-conclusion)

## Abstract

Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior—for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.

## 1. Introduction

Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior—for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks.

<details>
<summary>We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work.</summary>

We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.
</details>

## 2. Related Work

### Learning from Noisy Labels with Deep Neural Networks: A Survey

**Abstract**
Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we first describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 62 state-of-the-art robust training methods, all of which are categorized into five groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we perform an in-depth analysis of noise rate estimation and summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies.

### Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution

**Abstract**
When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR → STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).

### Exploring The Landscape of Distributional Robustness for Question Answering Models

**Abstract**
We conduct a large empirical evaluation to investigate the landscape of distributional robustness in question answering. Our investigation spans over 350 models and 16 question answering datasets, including a diverse set of architectures, model sizes, and adaptation methods (e.g., fine-tuning, adapter tuning, in-context learning, etc.). We find that, in many cases, model variations do not affect robustness and in-distribution performance alone determines out-of-distribution performance. Moreover, our findings indicate that i) zero-shot and in-context learning methods are more robust to distribution shifts than fully fine-tuned models; ii) few-shot prompt fine-tuned models exhibit better robustness than few-shot fine-tuned span prediction models; iii) parameter-efficient and robustness enhancing training methods provide no significant robustness improvements. In addition, we publicly release all evaluations to encourage researchers to further analyze robustness trends for question answering models.

### The case for aligning narrowly superhuman models

**Abstract**
I wrote this post to get people's takes on a type of work that seems exciting to me personally; I'm not speaking for Open Phil as a whole. Institutionally, we are very uncertain whether to prioritize this (and if we do where it should be housed and how our giving should be structured). We are not seeking grant applications on this topic right now.

### Artificial Sandwiching: When can we test scalable alignment protocols without humans

**Abstract**
Background: Standard sandwiching (in my terms)

Artificial sandwiching: Replacing the non-expert human

Epistemic status: Not a fleshed-out proposal. Brainstorming/eliciting ideas.

Thanks to Ben Mann, Pablo Moreno, and Jared Kaplan for feedback on early drafts.

I'm convinced sandwiching—the experimental protocol from Ajeya Cotra's The case for aligning narrowly superhuman models—is valuable, and I'm in the process of setting up some concrete sandwiching experiments to test scalable oversight ideas.

Sandwiching experiments are generally fairly slow:

- You have to design and pilot a strategy that allows humans to use (or oversee) a model for a task that they can't do well themselves. The details matter here, and this can often take many iterations to get right.

- Then, you need a bunch of humans actually try this. Even for very simple tasks, this is a high-cognitive-load task that should take at least tens of minutes per instance.

- You have to repeat this enough times to measure average performance accurately.

I'm visiting Anthropic this year for a sabbatical, and some of my sandwiching work is happening there. Anthropic's biggest comparative advantage (like that of similar teams at DeepMind and OpenAI) is easy access to near-state-of-the-art LMs that are fine-tuned to be helpful dialog agents.

In that context, I've heard or encountered this question several times: Can we speed up [some experiment I'm proposing] by replacing the non-expert human with a weaker LM?

This obviously doesn't achieve the full aims of sandwiching in general, but it's often hard to find a decisive rebuttal for these individual instances.

More broadly, I think there's likely to be a significant subset of worthwhile sandwiching experiments that can be trialed more quickly by using an intentionally weakened model as a proxy for the human.

Which experiments these are, precisely, has been hard for me to pin down. This post is an attempt to organize my thoughts and solicit comments.

## 3. Methodology

A core challenge of superalignment is that humans will need to supervise models much smarter than us. This is a special case of what we call the weak-to-strong learning problem: how can a weak supervisor oversee a model much smarter than it? In this paper, we study a simple analogy, in which we replace the weak human supervisor with a weak model supervisor.

For a given task of interest, consisting of a dataset and a performance metric, we:

1. Create the weak supervisor. Throughout most of this work, we create weak supervisors by finetuning small pretrained models on ground truth labels.
2. Train a strong student model with weak supervision. We finetune a strong model with the generated weak labels.
3. Train a strong model with ground truth labels as a ceiling. Finally, for comparison, we finetune a strong model with ground truth labels.

Typically, weak-to-strong performance will be between weak performance and strong ceiling performance. We define the performance gap recovered (PGR) as a function of the above three performances (weak, weak-to-strong, and strong ceiling) as shown in the illustration below.

PGR measures the fraction of the performance gap (the difference in performance between the weak and strong ceiling models) that we can recover with weak supervision. If we achieve perfect weak-to-strong generalization, PGR is 1. If the weak-to-strong model does no better than the weak supervisor, then PGR is 0.

## 4. Main Results

### 4.1 Tasks

**Popular natural language processing benchmarks**
We consider 22 popular NLP classification datasets covering ethics, commonsense reasoning, natural language inference, sentiment analysis, and other domains. We convert all datasets to binary classification tasks and approximately balance the classes. We produce soft labels from the weak model.

**Chess puzzles**
We use the dataset originally introduced in Schwarzschild et al. (2021b), which contains chess puzzles from the lichess.org website (Lichess Team, 2023). Each puzzle consists of a chess position, and a sequence of optimal moves to play to solve the puzzle. For our evaluation, we predict the first move played, which is the best move in the given chess position. For weak labels, we sample from the weak model with temperature 0.

**ChatGPT reward modeling**
The standard approach to aligning models today is reinforcement learning from human feedback (RLHF). A critical step of RLHF is to train a reward model (RM) to predict human preferences between model responses. We use a proprietary dataset used to train ChatGPT reward models.

### 4.2 Baseline Results

<details>
<summary>In general, across all our settings, we observe weak-to-strong generalization: strong students consistently outperform their weak supervisors. It is not obvious why this should happen at all—especially from naive finetuning alone—and it gives us hope that weak-to-strong learning is a tractable problem. At the same time, our results suggest that naively using weak, human-level supervision will be insufficient to align strong, superhuman models; we will need qualitatively new techniques to solve superalignment.</summary>

On the popular NLP benchmarks, we find especially promising weak-to-strong generalization: strong models trained with weak supervision can often generalize to a substantially higher performance than the weak model itself. The PGR increases both with weak supervisor size and with strong student size; for the largest students, the PGR is often above 50%.

We see more mixed results in the chess puzzle setting. In particular, when using the smallest weak models, the PGR is close to zero and the test accuracy curves appear flat. However, as the size of the weak supervisor increases, the PGR increases substantially; for small supervisor-student gaps, PGR can be above 40%. Unlike in the NLP setting, where PGR improves with the strong student size, PGR decreases with the strong student size for a given weak supervisor on chess puzzles.

Finally, we find that weak-to-strong generalization is poor by default in the ChatGPT reward model setting. We are usually only able to recover roughly 10% of the performance
