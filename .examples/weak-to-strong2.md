# Weak-to-Strong Generalization: Eliciting

## Table of Contents
- [Weak-to-Strong Generalization: Eliciting](#weak-to-strong-generalization-eliciting)
  - [Table of Contents](#table-of-contents)
  - [Abstract](#abstract)
  - [1. Weak Supervision and Superhuman Models](#1-weak-supervision-and-superhuman-models)
  - [2. Methodology](#2-methodology)
  - [3. Main Results](#3-main-results)
    - [3.1 Tasks](#31-tasks)
    - [3.2 Weak-to-Strong Generalization Baselines](#32-weak-to-strong-generalization-baselines)
    - [3.3 Improving Weak-to-Strong Generalization is Tractable](#33-improving-weak-to-strong-generalization-is-tractable)
  - [4. Understanding Weak-to-Strong Generalization](#4-understanding-weak-to-strong-generalization)
    - [4.1 Understanding Imitation](#41-understanding-imitation)
    - [4.2 Saliency in the Strong Model Representations](#42-saliency-in-the-strong-model-representations)
  - [5. Discussion](#5-discussion)
    - [5.1 Remaining Disanalogies](#51-remaining-disanalogies)
    - [5.2 Future Work](#52-future-work)
    - [5.3 Conclusion](#53-conclusion)

## Abstract

Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior—for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.

## 1. Weak Supervision and Superhuman Models

We mainly steer or align today's models with reinforcement learning from human feedback (RLHF): we reinforce behaviors that human evaluators rate highly and penalize behaviors that evaluators rate poorly. This procedure is very effective when human evaluators can tell if model behavior is good or bad and is a core part of training modern language model assistants such as ChatGPT.

However, superhuman models will be capable of complex and creative behaviors that humans cannot fully understand. For example, if a superhuman assistant model generates a million lines of extremely complicated code, humans will not be able to provide reliable supervision for key alignment-relevant tasks, including: whether the code follows the user's intentions, whether the assistant model answers questions about the code honestly, whether the code is safe or dangerous to execute, and so on. As a result, if we finetune a superhuman model with human supervision on a reward modeling (RM) or safety classification task, it is unclear how that model will generalize to complicated behaviors that humans could not reliably supervise themselves.

This leads to a fundamental technical challenge of aligning superhuman models (superalignment): how can weak supervisors control models much smarter than them?

We propose a simple setup for studying the problem of humans supervising superhuman models by considering an analogy: can we use weak models to supervise strong models? We can empirically test this by finetuning large (strong) pretrained models on labels generated by small (weak) models and observing how they generalize. Just like the problem of humans supervising superhuman models, our setup is an instance of what we call the weak-to-strong learning problem.

## 2. Methodology

We study our weak-to-strong learning setup (Section 3) by finetuning base (i.e. pretrained-only) language models from the GPT-4 family (OpenAI, 2023),1 spanning 7 orders of magnitude (OOMs) of pretraining compute, across three settings: a large set of popular natural language processing (NLP) benchmarks, chess puzzles, and our internal ChatGPT reward modeling dataset.

Our main findings include:

<details>
<summary>1. Strong pretrained models naturally generalize beyond their weak supervisors.</summary>
If we naively finetune strong models with labels generated by weak models, they consistently outperform their weak supervisors (Section 4.2). For example, on NLP tasks, if we finetune GPT-4 with labels from a GPT-2-level model, we typically recover about half of the performance gap between the two models.
</details>

<details>
<summary>2. Naively finetuning on weak supervision is not enough.</summary>
Despite positive weak-to-strong generalization, there still remains a substantial gap between strong models finetuned with weak supervision and strong models finetuned with ground truth supervision. Weak-to-strong generalization is particularly poor for ChatGPT reward modeling. Collectively, our results provide empirical evidence that naive RLHF will likely scale poorly to superhuman models without additional work.
</details>

<details>
<summary>3. Improving weak-to-strong generalization is tractable.</summary>
We find that we can improve performance by encouraging strong models to have confident predictions with an auxiliary loss, bootstrapping supervision with intermediate models, and improving model representations with unsupervised finetuning. For example, when supervising GPT-4 with a GPT-2-level model on NLP tasks using the auxiliary confidence loss, we typically recover nearly 80% of the performance gap between the weak and strong models.
</details>

We show that substantial weak-to-strong generalization is not only possible, but actually a widespread phenomenon. We also show that with very simple methods, we can drastically improve the ability of weak supervisors to elicit knowledge from strong models. With much more progress in this direction, we could get to the point where we can use weak supervisors to reliably elicit knowledge from much stronger models, at least for some key tasks that we care about. This may allow us to develop superhuman reward models or safety classifiers, which we could in turn use to align superhuman models.

## 3. Main Results

### 3.1 Tasks

**Popular natural language processing benchmarks.** We consider 22 popular NLP classification datasets covering ethics, commonsense reasoning, natural language inference, sentiment analysis, and other domains. We convert all datasets to binary classification tasks and approximately balance the classes. We produce soft labels from the weak model.

**Chess puzzles.** We use the dataset originally introduced in Schwarzschild et al. (2021b), which contains chess puzzles from the lichess.org website (Lichess Team, 2023). Each puzzle consists of a chess position, and a sequence of optimal moves to play to solve the puzzle. For our evaluation, we predict the first move played, which is the best move in the given chess position. For weak labels, we sample from the weak model with temperature 0.

**ChatGPT reward modeling.** The standard approach to aligning models today is reinforcement learning from human feedback (RLHF). A critical step of RLHF is to train a reward model (RM) to predict human preferences between model responses. We use a proprietary dataset used to train ChatGPT reward models.

### 3.2 Weak-to-Strong Generalization Baselines

On the popular NLP benchmarks, we find especially promising weak-to-strong generalization: strong models trained with weak supervision can often generalize to a substantially higher performance than the weak model itself.

We see more mixed results in the chess puzzle setting. In particular, when using the smallest weak models, the PGR is close to zero and the test accuracy curves appear flat. However, as the size of the weak supervisor increases, the PGR increases substantially; for small supervisor-student gaps, PGR can be above 40%.

Finally, we find that weak-to-strong generalization is poor by default in the ChatGPT reward model setting. We are usually only able to recover roughly 10% of the performance gap between the weak supervisor and the strong student.

In general, across all our settings, we observe weak-to-strong generalization: strong students consistently outperform their weak supervisors. It is not obvious why this should happen at all—especially from naive finetuning alone—and it gives us hope that weak-to-strong learning is a tractable problem. At the same time, our results suggest that naively using weak, human-level supervision will be insufficient to align strong, superhuman models; we will need qualitatively new techniques to solve superalignment.

### 3.3 Improving Weak-to-Strong Generalization is Tractable

We now show that we can use simple methods to substantially improve weak-to-strong generalization.

<details>
<summary>Bootstrapping with Intermediate Model Sizes</summary>
Bootstrapping is a long-standing idea in alignment: instead of directly aligning very superhuman models, we could first align an only slightly superhuman model, use that to align an even smarter model, and so on (Christiano, 2019; 2018; Leike & Sutskever, 2023; Worley, 2021). Our setting allows us to empirically test this idea.

We report the results (including all intermediate weak-to-strong models within each bootstrap) in Figure 4. Bootstrapping improves PGR compared to the baseline, especially for larger student models. With the naive method, transfer accuracy curves flatten as the weak-strong gap grows larger; with bootstrapping, the accuracy continues to monotonically improve.
</details>

<details>
<summary>An Auxiliary Confidence Loss Can Dramatically Improve Generalization on NLP Tasks</summary>
In our baseline results (Section 4.2), we naively finetune the strong student on the labels provided by the weak supervisor. Because we are directly training the strong student to imitate the weak supervisor, it may also learn to imitate the errors of the supervisor (see Section 5.1 for more discussion).

We operationalize this intuition by adding an auxiliary confidence loss term to the standard cross entropy objective. This method is closely related to conditional entropy minimization (Grandvalet & Bengio, 2004) which is a prominent technique in semi-supervised learning. Specifically, we add an additional loss term which reinforces the strong model's confidence in its own predictions—even when they disagree with the weak labels.

In Figure 5, we plot accuracy and PGR curves with this method on our NLP tasks. We find that while it performs slightly worse than the naive baseline for smaller strong students, it dramatically improves generalization for large gaps in compute between weak and strong models.
</details>

## 4. Understanding Weak-to-Strong Generalization

Strong methods will be essential for solving superalignment, but to trust those methods it is also important to understand when and why they work. A better understanding of weak-to-strong generalization could help us trust that generalization will continue working even in the future high-stakes settings we care most about, and could help us develop better methods along the way.

### 4.1 Understanding Imitation

When we train a strong model with weak supervision on some task, our hope is that the strong model will perform that desired task as well as possible, leveraging the latent capabilities it learned from pretraining to significantly outperform the weak supervisor. A salient way in which we could fail to achieve that desired generalization is if the strong model instead learns to imitate the weak supervisor—predicting how the weak supervisor would have classified each example.

<details>
<summary>Overfitting to Weak Supervision</summary>
Empirically, we see that the strong student indeed appears to overfit to the weak supervisor's errors. In Figure 7(a) we show ground truth test accuracy curves over the course of training for the ChatGPT RM task, and in Figure 7(b) and (c) we compare the best and final ground truth test accuracies (median across all weak-strong model pairs). We find overfitting for large weak-strong gaps. For small weak-strong gaps, weak-to-strong performance typically monotonically increases over the course of training. For larger gaps, weak-to-strong performance often increases initially, but then starts dropping well before a single epoch has elapsed.
</details>

<details>
<summary>Student-Supervisor Agreement</summary>
Another way to measure imitation is to directly measure the agreement between the student and the supervisor: we notice that for our naive finetuning baseline, student-supervisor agreement is consistently high—often noticeably higher than weak supervisor accuracy. This indicates that the student is imitating some of the supervisor's errors. These phenomena hold across all tasks (NLP tasks, chess, and reward modeling) and all model sizes, for the naive method.

The confidence loss in Section 4.3.2 reduces student-supervisor agreements significantly (Figure 8), primarily by imitating supervisor mistakes less (Figure 8c). The loss encourages the strong student to make confident predictions, including when they contradict the weak supervisor. In a handful of the settings where it is most successful, the confidence loss reduces student-supervisor agreement below strong student test accuracy (weak-to-strong performance)—i.e., the resulting model is fitting the ground truth concept better than it is fitting the weak labels it was trained with.
</details>

<details>
<summary>Inverse Scaling for Imitating the Supervisor</summary>
Next, we study student-supervisor agreement as a function strong model size (see Figure 8 and Figure 16). Surprisingly, we find inverse scaling (McKenzie et al., 2023): larger student models consistently agree less with the errors of the supervisor than smaller student models, despite being trained to imitate the supervisor, not using early stopping, and having larger capacity than smaller student models.
</details>

### 4.2 Saliency in the Strong Model Representations

One intuition for when weak-to-strong generalization might be feasible is when the task or concept we want to elicit is internally "salient" to the strong model. In this section, we study several phenomena related to the saliency of the concepts we are trying to elicit from the student model.

<details>
<summary>Eliciting Strong Model Knowledge with Prompting</summary>
One possible reason for the high PGR we observe in Section 4 could be that eliciting what the strong model knows is easy. In particular, it is possible that strong pretrained models can solve many relevant tasks zero-shot with a simple prompt.

In Figure 9a, we consider 7 representative NLP tasks and compare finetuning, zero-shot prompting, and 5-shot prompting; for this initial experiment, we use ground truth labels rather than weak labels. We find that zero-shot and 5-shot test accuracy is poor for most model sizes but, consistent with Brown et al. (2020), improves drastically for larger model sizes. In particular, for the largest models, 5-shot prompting becomes competitive with finetuning on many tasks, indicating that eliciting the task-relevant knowledge of these very large models is relatively straightforward.
</details>

<details>
<summary>Generative Supervision Improves RM Weak-to-Strong Generalization</summary>
If salient representations of the desired task is useful for weak-to-strong generalization, then we may be able to improve generalization by increasing the salience of the task to the strong model. One way to increase the salience of a task without needing ground truth labels is to perform unsupervised finetuning with the language modeling objective on data relevant to that task (Dai & Le, 2015). We test this idea in our reward modeling setting, where it is standard practice to initialize the model with a baseline finetuned on demonstrations of desired behaviors (Stiennon et al., 2020). We found that the additional generative finetuning on the RM data leads to better weak-to-strong performance.
</details>

<details>
<summary>Finetuning on Weak Supervision to Increase Concept Saliency</summary>
One possible measure of concept saliency is how linearly represented a task is. In particular, we can measure the performance of a linear probe (logistic regression classifier) trained from frozen activations of the model. If the optimal solution can be approximately recovered with a linear probe, that could simplify our problem greatly; we could focus on linear probing methods instead of finetuning methods, which could greatly reduce the search space we need to consider to elicit the desired generalization.

We show that when we finetune the strong model with weak labels, the representations become more linear even with respect to ground truth labels. In fact, finetuning on weak labels then linear probing on ground truth labels results in an accuracy of 78%, closing 60% of the gap between ground truth linear probing and finetuning.
</details>

## 5. Discussion
Sure, here's the continued discussion from the document:

### 5.1 Remaining Disanalogies

The document discusses two key disanalogies between the weak-to-strong learning setup and the problem of aligning superhuman models:

<details>
<summary>Imitation Saliency</summary>
Superhuman models may easily imitate weak errors. The document notes that if we naively train such a superhuman model with human supervision, it might simply imitate the weak supervisor, outputting human-level capabilities rather than its latent superhuman capabilities.
</details>

<details>
<summary>Pretraining Leakage</summary>
Superhuman knowledge may be latent, not observable. The document notes that many of the tasks considered in the work may have been observed in pretraining at least indirectly. However, future superhuman models may never directly observe superhuman alignment-relevant capabilities; these capabilities may be predominantly "latent", e.g. learned through self-supervised learning or reinforcement learning rather than through imitation learning. Intuitively, latent capabilities may be harder to elicit than capabilities that models could have observed in their pretraining data.
</details>

### 5.2 Future Work

The document outlines several concrete problems for future work to address the remaining disanalogies and make progress towards solving the superalignment problem:

<details>
<summary>Concrete Problems: Analogous Setups</summary>
- Making the setup more analogous by fixing the main remaining disanalogies
- Validating that disanalogies are not severe, e.g. by checking that results are qualitatively similar to using 3rd grade humans to supervise the strongest models today
- Relaxing simplifications, e.g. by generalizing methods and results to complicated generative tasks
- Testing the robustness of weak-to-strong classifiers to optimization pressure
- Testing the conjecture that prompting-based methods will not be as indicative of future results relative to finetuning-based methods
</details>

<details>
<summary>Concrete Problems: Scalable Methods</summary>
- Identifying properties that the desired generalization should satisfy, such as being able to disagree with the weak supervision when it is wrong, being "natural" or "salient" to the model, and being consistent
- Exploring ways to reliably estimate generalization error at test time without any labels, e.g. by measuring weak-to-strong underspecification
- Developing scalable methods that can reliably extrapolate generalization error across many orders of magnitude using scaling laws
</details>

<details>
<summary>Concrete Problems: Scientific Understanding</summary>
- Understanding what makes a concept easy or hard to elicit, and defining "salience" precisely
- Developing reliable ways to extrapolate generalization error across many orders of magnitude using scaling laws
</details>

### 5.3 Conclusion

The document concludes by emphasizing the importance of establishing extremely high reliability in the alignment of superhuman models, as the possibility of such models being developed this decade has become increasingly plausible. It suggests that the weak-to-strong learning setup provides a valuable tool for making empirical progress on this fundamental challenge, and outlines concrete problems for future work to address the remaining disanalogies and develop scalable, reliable methods for aligning superhuman models.
